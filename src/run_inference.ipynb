{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d80798e-20bd-401f-a156-9577573273fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import quantile_transform \n",
    "from x_transformers import XTransformer, TransformerWrapper, Decoder, Encoder, ViTransformerWrapper\n",
    "from dataset import DNASequenceDataSet\n",
    "from model import DNATransformerEncoder, get_model\n",
    "from training_utils import train_model, validate_model, test_model\n",
    "from data_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac6345-11c5-44bb-8ffc-56c3186c09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as stream:\n",
    "    try:\n",
    "        cfg = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4033704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer here as you did during training\n",
    "device = torch.device(\"cuda:0\")  \n",
    "cfg.update({\n",
    "    'device': device,\n",
    "})\n",
    "\n",
    "model = get_model(cfg).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg['learning_rate'])\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(cfg['checkpoint_path'])  # Replace X with the epoch number\n",
    "\n",
    "# Restore the model and optimizer states\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Move the model to evaluation mode if you are doing inference\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04b462c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, embed, tgt = model(df.Sequence[:2].tolist(), df.Sequence[:2].tolist())\n",
    "\n",
    "embed[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c8ff4-c119-4ce9-ac51-b317671f30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_preprocess_enrichment_data(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34c40474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('../data/saved_h5/dna_dataset_classification.h5', 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8049584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 20  # Replace with your value of N\n",
    "\n",
    "# Regular expression pattern to match more than N consecutive 'G's\n",
    "pattern = f'C{{{N},}}'\n",
    "\n",
    "# Filter sequences with more than N consecutive 'G's\n",
    "sequences_with_consecutive_Gs = df[df['Sequence'].str.contains(pattern)]\n",
    "\n",
    "print(len(sequences_with_consecutive_Gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"N\"), )\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.mask_token = \"[MASK]\"\n",
    "tokenizer.cls_token = \"[CLS]\"\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.unknown_token = \"N\"\n",
    "tokenizer.model_max_length = 42\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A\",\n",
    "    special_tokens=[(\"[PAD]\",0), (\"N\", 1), (\"[CLS]\", 2), (\"[MASK]\", 3)]\n",
    ")\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=8,\n",
    "    special_tokens=[\"[PAD]\", \"N\", \"[CLS]\", \"[MASK]\"],\n",
    ")\n",
    "\n",
    "temp_df = df.Sequence.apply(lambda x:\" \".join(x))\n",
    "tokenizer.train_from_iterator(temp_df.values, trainer=trainer)\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer, model_max_length=42, cls_token=\"[CLS]\", unk_token=\"N\", pad_token=\"[PAD]\", mask_token=\"[MASK]\", return_special_tokens_mask=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e70350",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer.save_pretrained('../data/AptamerBERT_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfcb825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "tokenizer = AutoTokenizer.from_pretrained('../data/AptamerBERT_tokenizer')\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.Sequence.apply(lambda x:\" \".join(x))\n",
    "\n",
    "batched_data = temp_df.values.tolist()[:1000]\n",
    "tokenized_batch = tokenizer(batched_data, padding=True)\n",
    "masked_data = data_collator(tokenized_batch.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg['device'] = device\n",
    "\n",
    "model = x_transformer_encoder= TransformerWrapper(\n",
    "    num_tokens = 8,\n",
    "    max_seq_len = 42,\n",
    "    num_memory_tokens = cfg['num_memory_tokens'],\n",
    "    l2norm_embed = cfg['l2norm_embed'],\n",
    "    attn_layers = Encoder(\n",
    "        dim = cfg['d_model'],\n",
    "        depth = cfg['num_layers'],\n",
    "        heads = cfg['nhead'],\n",
    "        layer_dropout = cfg['dropout_rate'],   # stochastic depth - dropout entire layer\n",
    "        attn_dropout = cfg['dropout_rate'],    # dropout post-attention\n",
    "        ff_dropout = cfg['dropout_rate'],       # feedforward dropout,\n",
    "        attn_flash = cfg['attn_flash'],\n",
    "        attn_num_mem_kv = cfg['attn_num_mem_kv'],\n",
    "        use_scalenorm = cfg['use_scalenorm'],\n",
    "        use_simple_rmsnorm = cfg['use_simple_rmsnorm'],\n",
    "        ff_glu = cfg['ff_glu'],\n",
    "        ff_swish = cfg['ff_swish'],\n",
    "        ff_no_bias = cfg['ff_no_bias'],\n",
    "        attn_talking_heads = cfg['attn_talking_heads']\n",
    "    )\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "src = masked_data['input_ids'].to(device)\n",
    "src_mask = torch.Tensor(tokenized_batch.attention_mask).bool().to(device)\n",
    "\n",
    "trg = masked_data['labels'].to(device)\n",
    "\n",
    "\n",
    "out = model(src, mask=src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ade01",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(out.movedim(2,1), trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbeb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg['device'] = device\n",
    "\n",
    "model = XTransformer(\n",
    "    dim = cfg['d_model'],\n",
    "    enc_num_tokens = 8,\n",
    "    enc_depth = cfg['num_layers'],\n",
    "    enc_heads = 8,\n",
    "    enc_max_seq_len = 42,\n",
    "    dec_num_tokens = 8,\n",
    "    dec_depth = cfg['num_layers'],\n",
    "    dec_heads = 8,\n",
    "    dec_max_seq_len = 42,\n",
    "    tie_token_emb = True      # tie embeddings of encoder and decoder\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "src = masked_data['input_ids'].to(device)\n",
    "src_mask = torch.Tensor(tokenized_batch.attention_mask).bool().to(device)\n",
    "\n",
    "trg = torch.Tensor(tokenized_batch.input_ids).long().to(device)\n",
    "\n",
    "\n",
    "out = model(src, trg, mask=src_mask)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695c9dc-6af0-4a85-8296-d428711cf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='Normalized_Frequency', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5aebec-f857-4c13-b58e-4a1f8dbd3362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npl-2023b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
