{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d80798e-20bd-401f-a156-9577573273fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-09 21:03:22.870751: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-09 21:03:22.911537: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-09 21:03:22.911577: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-09 21:03:22.912988: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-09 21:03:22.920902: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-09 21:03:23.694473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import quantile_transform \n",
    "from x_transformers import XTransformer, TransformerWrapper, Decoder, Encoder, ViTransformerWrapper\n",
    "\n",
    "from model import *\n",
    "from factories_model_loss import *\n",
    "from data_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ac6345-11c5-44bb-8ffc-56c3186c09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = read_cfg('config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4033704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizer here as you did during training\n",
    "device = torch.device(\"cuda:0\")  \n",
    "cfg.update({\n",
    "    'device': device,\n",
    "})\n",
    "\n",
    "model = get_model(cfg).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg['learning_rate'])\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(cfg['checkpoint_path'])  # Replace X with the epoch number\n",
    "\n",
    "# Restore the model and optimizer states\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# Move the model to evaluation mode if you are doing inference\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04b462c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, embed, tgt = model(df.Sequence[:2].tolist(), df.Sequence[:2].tolist())\n",
    "\n",
    "embed[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c8ff4-c119-4ce9-ac51-b317671f30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_preprocess_enrichment_data(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34c40474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf('../data/saved_h5/dna_dataset_classification.h5', 'df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b09fd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_dataset = DNAEncoderDataSet(df, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2587d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d41b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(dna_dataset, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68c4ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'rb') as f:\n",
    "    # The protocol version used is detected automatically, so we do not\n",
    "    # have to specify it.\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44618deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full([max_seq_len, max_seq_len], float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8049584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 20  # Replace with your value of N\n",
    "\n",
    "# Regular expression pattern to match more than N consecutive 'G's\n",
    "pattern = f'C{{{N},}}'\n",
    "\n",
    "# Filter sequences with more than N consecutive 'G's\n",
    "sequences_with_consecutive_Gs = df[df['Sequence'].str.contains(pattern)]\n",
    "\n",
    "print(len(sequences_with_consecutive_Gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"N\"), )\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.mask_token = \"[MASK]\"\n",
    "tokenizer.cls_token = \"[CLS]\"\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.unknown_token = \"N\"\n",
    "tokenizer.model_max_length = 42\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A\",\n",
    "    special_tokens=[(\"[PAD]\",0), (\"N\", 1), (\"[CLS]\", 2), (\"[MASK]\", 3)]\n",
    ")\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=8,\n",
    "    special_tokens=[\"[PAD]\", \"N\", \"[CLS]\", \"[MASK]\"],\n",
    ")\n",
    "\n",
    "temp_df = df.Sequence.apply(lambda x:\" \".join(x))\n",
    "tokenizer.train_from_iterator(temp_df.values, trainer=trainer)\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer, model_max_length=42, cls_token=\"[CLS]\", unk_token=\"N\", pad_token=\"[PAD]\", mask_token=\"[MASK]\", return_special_tokens_mask=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e70350",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer.save_pretrained('../data/AptamerBERT_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edfcb825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='../data/AptamerBERT_tokenizer', vocab_size=8, model_max_length=42, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': 'N', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"N\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "tokenizer = AutoTokenizer.from_pretrained('../data/AptamerBERT_tokenizer')\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81ea595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.Sequence.apply(lambda x:\" \".join(x))\n",
    "\n",
    "batched_data = temp_df.values.tolist()[:1000]\n",
    "tokenized_batch = tokenizer(batched_data, padding=True, )\n",
    "\n",
    "masked_data = data_collator(tokenized_batch.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a1bfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False,  True],\n",
       "        [False, False, False,  ..., False, False,  True],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False,  True],\n",
       "        [False, False, False,  ..., False, False,  True],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "~torch.Tensor(tokenized_batch['attention_mask']).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a4471",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg['device'] = device\n",
    "\n",
    "model = x_transformer_encoder= TransformerWrapper(\n",
    "    num_tokens = 8,\n",
    "    max_seq_len = 42,\n",
    "    num_memory_tokens = cfg['num_memory_tokens'],\n",
    "    l2norm_embed = cfg['l2norm_embed'],\n",
    "    attn_layers = Encoder(\n",
    "        dim = cfg['d_model'],\n",
    "        depth = cfg['num_layers'],\n",
    "        heads = cfg['nhead'],\n",
    "        layer_dropout = cfg['dropout_rate'],   # stochastic depth - dropout entire layer\n",
    "        attn_dropout = cfg['dropout_rate'],    # dropout post-attention\n",
    "        ff_dropout = cfg['dropout_rate'],       # feedforward dropout,\n",
    "        attn_flash = cfg['attn_flash'],\n",
    "        attn_num_mem_kv = cfg['attn_num_mem_kv'],\n",
    "        use_scalenorm = cfg['use_scalenorm'],\n",
    "        use_simple_rmsnorm = cfg['use_simple_rmsnorm'],\n",
    "        ff_glu = cfg['ff_glu'],\n",
    "        ff_swish = cfg['ff_swish'],\n",
    "        ff_no_bias = cfg['ff_no_bias'],\n",
    "        attn_talking_heads = cfg['attn_talking_heads']\n",
    "    )\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "src = masked_data['input_ids'].to(device)\n",
    "src_mask = torch.Tensor(tokenized_batch.attention_mask).bool().to(device)\n",
    "\n",
    "trg = masked_data['labels'].to(device)\n",
    "\n",
    "\n",
    "out = model(src, mask=src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ade01",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy(out.movedim(2,1), trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbeb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "cfg['device'] = device\n",
    "\n",
    "model = XTransformer(\n",
    "    dim = cfg['d_model'],\n",
    "    enc_num_tokens = 8,\n",
    "    enc_depth = cfg['num_layers'],\n",
    "    enc_heads = 8,\n",
    "    enc_max_seq_len = 42,\n",
    "    dec_num_tokens = 8,\n",
    "    dec_depth = cfg['num_layers'],\n",
    "    dec_heads = 8,\n",
    "    dec_max_seq_len = 42,\n",
    "    tie_token_emb = True      # tie embeddings of encoder and decoder\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "src = masked_data['input_ids'].to(device)\n",
    "src_mask = torch.Tensor(tokenized_batch.attention_mask).bool().to(device)\n",
    "\n",
    "trg = torch.Tensor(tokenized_batch.input_ids).long().to(device)\n",
    "\n",
    "\n",
    "out = model(src, trg, mask=src_mask)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695c9dc-6af0-4a85-8296-d428711cf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='Normalized_Frequency', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5aebec-f857-4c13-b58e-4a1f8dbd3362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npl-2023b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
