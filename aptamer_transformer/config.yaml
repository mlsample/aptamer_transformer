# Model used
model_type: "seq_struct_x_aptamer_bert"  


#Path parameters
working_dir: '/scratch/mlsample/research/project_files/aptamer_transformer'
checkpoint_path: '{WORKING_DIR}/results/{MODEL_TYPE}/{MODEL_TYPE}_checkpoint.pt'
results_path: '{WORKING_DIR}/results/{MODEL_TYPE}'


#Logging parameters
load_last_checkpoint: False
running_echo: False

# Data_set: 'sars-cov2-raw'
save_data_set: False
load_saved_data_set: True
load_saved_df: '{WORKING_DIR}/data/saved_processed_data/saved_dfs/log_normed_thresh5.pickle'

# Training data parameters
seed_value: 42
num_epochs: 5
batch_size: 8096
num_workers: 4


# Transformer parameters
num_layers: 1
d_model: 8
d_ff: 64
dropout_rate: 0.4
nhead: 8
num_classes: 2 # Only valid if model_task is classification or evidence and load_saved_data_set is False  

# Optimizer parameters
learning_rate: 0.001
lr_patience: 3
lr_scheduler: ReduceLROnPlateau


# Early stopping parameters
stopping_patience: 6
min_delta: 0

# AptamerBERT parameters
mlm_probability: 0.1
aptamer_bert_path: '{WORKING_DIR}/results/aptamer_bert/aptamer_bert_checkpoint_to_load.pt' #Current aptamer_bert checkpoint parameters are num_layers: 6, d_model: 64, nhead: 8, d_ff: 512, dropout_rate: 0.1  
x_aptamer_bert_path: '{WORKING_DIR}/results/x_aptamer_bert/x_aptamer_bert_checkpoint.pt'
seq_struct_aptamer_bert_path: '{WORKING_DIR}/results/seq_struct_x_aptamer_bert_loader' # num_layers: 250, d_model: 120, nhead: 2, d_ff: 2, dropout_rate: 0.4

# Tokenizers
seq_tokenizer_path: '{WORKING_DIR}/data/tokenizers/AptamerBERT_tokenizer'
dot_bracket_tokenizer_path: '{WORKING_DIR}/data/tokenizers/dot_bracket_struct_tokenizer'
seq_struct_tokenizer_path: '{WORKING_DIR}/data/tokenizers/seq_struct_sep_whitespace'


# Data parameters
classification_threshold: 0.5
norm_2: 'quantile_transform'
n_quantiles: 1000
round_weighting: False # Only valid if load_saved_data_set is False
agg_type: 'mean'
log_normed: True
debug: False
data_directory: '{WORKING_DIR}/data/seq_data/sars-cov2-raw'


# X_transformer parameters
attn_flash: False
attn_num_mem_kv: 2
num_memory_tokens: 12
use_scalenorm: False
l2norm_embed: True
use_simple_rmsnorm: True
ff_glu: True
ff_swish: False
ff_no_bias: False
attn_talking_heads: False
attn_gate_values: False
macaron: False

rel_pos_bias: False
rotary_pos_emb: True
rotary_xpos: False
alibi_pos_bias: True

residual_attn: False
pre_norm: True

attn_qk_norm: False
attn_qk_norm_dim_scale: True

emb_dropout: 0.2
layer_dropout:  0.2
attn_dropout: 0.2
ff_dropout: 0.2


# transformer_encoder_regression
# transformer_encoder_classifier
# transformer_encoder_evidence
# dot_bracket_transformer_encoder_classifier

# x_transformer_encoder_regression
# x_transformer_encoder_classifier
# x_transformer_encoder_evidence

# aptamer_bert
# aptamer_bert_regression
# aptamer_bert_classifier
# aptamer_bert_evidence

# x_aptamer_bert
# x_aptamer_bert_classifier
# x_aptamer_bert_evidence
# x_aptamer_bert_regression

# struct_transformer_encoder_classifier
# struct_transformer_encoder_regression
# seq_struct_transformer_encoder_regression
# seq_struct_transformer_encoder_classifier
# seq_struct_aptamer_bert
# seq_struct_x_aptamer_bert
# seq_struct_x_aptamer_bert_regression
# seq_struct_x_aptamer_bert_classifier
# seq_struct_x_aptamer_bert_evidence

# seq_struct_ener_matrix_transformer_encoder_regression