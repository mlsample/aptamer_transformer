working_dir: '/glade/u/home/mlsample/work/aptamer_transformer'

#Logging parameters
load_last_checkpoint: False
checkpoint_path: '{WORKING_DIR}/results/{MODEL_TYPE}/{MODEL_TYPE}_checkpoint.pt'
results_path: '{WORKING_DIR}/results/{MODEL_TYPE}'
tensorboard_log_path: '{WORKING_DIR}/results/{MODEL_TYPE}/'

#model
model_type: "transformer_encoder_classifier"  

# transformer_encoder_regression
# transformer_encoder_classifier
# transformer_encoder_evidence
# dot_bracket_transformer_encoder_classifier

# x_transformer_encoder_regression
# x_transformer_encoder_classifier
# x_transformer_encoder_evidence

# aptamer_bert
# aptamer_bert_regression
# aptamer_bert_classifier
# aptamer_bert_evidence

# x_aptamer_bert
# x_aptamer_bert_classifier
# x_aptamer_bert_evidence
# x_aptamer_bert_regression

#training parameters
num_epochs: 100
seed_value: 41
batch_size: 4096
num_workers: 2
learning_rate: 0.0001
lr_patience: 6
lr_scheduler: ReduceLROnPlateau

#transformer parameters
num_classes: 2  # Invalid for regression task
num_layers: 6
d_model: 64
nhead: 8
d_ff: 512
dropout_rate: 0.1

#x_transformer parameters
attn_flash: True
attn_num_mem_kv: 0
num_memory_tokens: 0
use_scalenorm: False
l2norm_embed: False
use_simple_rmsnorm: False
ff_glu: True
ff_swish: False
ff_no_bias: True
attn_talking_heads: False
attn_gate_values: False
# sandwich_coef: 6
macaron: False

rel_pos_bias: False
rotary_pos_emb: True
rotary_xpos: False
alibi_pos_bias: False

residual_attn: False
pre_norm: False

attn_qk_norm: False
attn_qk_norm_dim_scale: False

emb_dropout: 0.1
layer_dropout: 0.1
attn_dropout: 0.1
ff_dropout: 0.1

#AptamerBERT parameters
tokenizer_path: '{WORKING_DIR}/data/tokenizers/AptamerBERT_tokenizer'
mlm_probability: 0.15
aptamer_bert_path: '{WORKING_DIR}/results/aptamer_bert/aptamer_bert_checkpoint.pt'
#Current aptamer_bert checkpoint parameters are num_layers: 6, d_model: 64, nhead: 8, d_ff: 512, dropout_rate: 0.1  
x_aptamer_bert_path: '{WORKING_DIR}/results/x_aptamer_bert/x_aptamer_bert_checkpoint.pt'
#data parameters
classification_threshold: 0.9
norm_2: 'quantile_transform'
n_quantiles: 1000
round_weighting: False # Only valid if load_saved_data_set is False

save_data_set: True
load_saved_data_set: False
debug: False
data_directory: '{WORKING_DIR}/data/seq_data/sars-cov2-raw'
