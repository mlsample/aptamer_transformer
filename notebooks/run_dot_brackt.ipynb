{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d80798e-20bd-401f-a156-9577573273fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 15:29:48.302150: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-17 15:29:48.302218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-17 15:29:48.303172: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-17 15:29:48.308767: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-17 15:29:48.997489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import quantile_transform \n",
    "from x_transformers import XTransformer, TransformerWrapper, Decoder, Encoder, ViTransformerWrapper\n",
    "\n",
    "from aptamer_transformer.model import *\n",
    "from aptamer_transformer.factories_model_loss import *\n",
    "from aptamer_transformer.data_utils import *\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880100f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/saved_processed_data/saved_h5/seq_struct_n_classes_2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e567bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dbf97ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "structures = df.dot_bracket_struc.apply(lambda x: ' '.join(x)).values.tolist()\n",
    "seqs = df.Sequence.apply(lambda x: ' '.join(x)).values.tolist()\n",
    "\n",
    "seq_structs = [f'{seq}{struct}' for seq, struct in zip(df.Sequence, df.dot_bracket_struc)]\n",
    "\n",
    "seq_structs_white_space = [(seq , struct) for seq, struct in zip(seqs, structures)]\n",
    "\n",
    "len_seq_structs = [len(struct) for struct in seq_structs]\n",
    "print(max(len_seq_structs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a694361",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_struct_tokenize = AutoTokenizer.from_pretrained(cfg['seq_tokenizer_path'])\n",
    "\n",
    "tokenized = seq_struct_tokenize(seqs[:10], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2620d923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 4, 7, 5, 6, 6, 6, 6, 6, 6, 7, 6, 6, 6, 7, 7, 7, 6, 6, 6, 7, 4,\n",
       "        7, 6, 6, 6, 6, 7, 5, 7, 6, 5, 4, 5, 7, 4, 7, 6, 6, 5, 7, 5],\n",
       "       [2, 7, 5, 6, 5, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 7, 5, 6, 6, 6, 7, 6,\n",
       "        5, 7, 5, 6, 7, 7, 5, 6, 4, 6, 6, 6, 6, 7, 5, 6, 5, 4, 6, 0],\n",
       "       [2, 4, 6, 6, 7, 7, 6, 6, 7, 6, 7, 4, 6, 6, 7, 7, 7, 4, 6, 6, 7, 7,\n",
       "        6, 7, 4, 7, 7, 6, 6, 7, 7, 4, 6, 6, 6, 7, 7, 6, 4, 6, 4, 0],\n",
       "       [2, 7, 6, 6, 5, 7, 6, 7, 5, 7, 7, 4, 5, 6, 4, 7, 5, 6, 5, 6, 4, 4,\n",
       "        6, 6, 4, 5, 6, 4, 7, 7, 6, 4, 4, 4, 4, 5, 7, 7, 7, 6, 4, 0],\n",
       "       [2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0],\n",
       "       [2, 4, 7, 4, 6, 7, 7, 4, 5, 6, 6, 6, 7, 6, 4, 6, 5, 5, 6, 7, 5, 4,\n",
       "        7, 7, 5, 4, 4, 6, 7, 7, 7, 4, 5, 7, 4, 5, 7, 7, 5, 7, 6, 0],\n",
       "       [2, 5, 5, 5, 4, 7, 6, 6, 7, 4, 6, 6, 7, 4, 7, 7, 6, 5, 7, 7, 6, 6,\n",
       "        7, 4, 6, 6, 6, 4, 7, 4, 6, 7, 6, 6, 6, 5, 7, 7, 6, 4, 7, 6],\n",
       "       [2, 6, 6, 6, 4, 6, 6, 6, 4, 6, 6, 6, 7, 6, 6, 6, 6, 6, 6, 7, 7, 5,\n",
       "        7, 5, 6, 5, 7, 6, 5, 6, 6, 6, 7, 7, 7, 7, 6, 6, 7, 6, 5, 0],\n",
       "       [2, 7, 6, 6, 6, 7, 6, 6, 6, 4, 6, 6, 6, 4, 6, 6, 6, 6, 6, 6, 5, 7,\n",
       "        7, 7, 4, 7, 7, 5, 5, 6, 6, 7, 6, 7, 7, 7, 7, 7, 7, 5, 6, 0],\n",
       "       [2, 4, 6, 6, 5, 7, 7, 4, 7, 5, 6, 7, 5, 4, 6, 6, 6, 6, 6, 6, 7, 6,\n",
       "        6, 6, 7, 5, 5, 6, 6, 6, 7, 7, 5, 6, 6, 6, 6, 7, 5, 6, 7, 0]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenized.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de22f853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] A T C G G G G G G T G G G T T T G G G T A T G G G G T C T G C A C T A T G G C T C',\n",
       " '[CLS] T C G C G G G G G G C G G G T C G G G T G C T C G T T C G A G G G G T C G C A G [PAD]',\n",
       " '[CLS] A G G T T G G T G T A G G T T T A G G T T G T A T T G G T T A G G G T T G A G A [PAD]',\n",
       " '[CLS] T G G C T G T C T T A C G A T C G C G A A G G A C G A T T G A A A A C T T T G A [PAD]',\n",
       " '[CLS] G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G [PAD]',\n",
       " '[CLS] A T A G T T A C G G G T G A G C C G T C A T T C A A G T T T A C T A C T T C T G [PAD]',\n",
       " '[CLS] C C C A T G G T A G G T A T T G C T T G G T A G G G A T A G T G G G C T T G A T G',\n",
       " '[CLS] G G G A G G G A G G G T G G G G G G T T C T C G C T G C G G G T T T T G G T G C [PAD]',\n",
       " '[CLS] T G G G T G G G A G G G A G G G G G G C T T T A T T C C G G T G T T T T T T C G [PAD]',\n",
       " '[CLS] A G G C T T A T C G T C A G G G G G G T G G G T C C G G G T T C G G G G T C G T [PAD]']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_struct_tokenize.batch_decode(tokenized.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13638af",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_structs_white_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "867c0d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 7, 10, 8, 9, 9, 9, 9, 9, 9, 10, 9, 9, 9, 10, 10, 10, 9, 9, 9, 10, 7, 10, 9, 9, 9, 9, 10, 8, 10, 9, 8, 7, 8, 10, 7, 10, 9, 9, 8, 10, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 4, 6, 6, 6, 6, 6, 6, 5, 5, 6, 6], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = read_cfg('../aptamer_transformer/config.yaml')\n",
    "\n",
    "seq_struct_tokenize = AutoTokenizer.from_pretrained(cfg['seq_struct_tokenizer_path'])\n",
    "\n",
    "seq_struct_tokenize(seq_structs_white_space[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0a8df7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq_structs_white_space' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train the tokenizer\u001b[39;00m\n\u001b[1;32m     22\u001b[0m trainer \u001b[38;5;241m=\u001b[39m BpeTrainer(special_tokens\u001b[38;5;241m=\u001b[39mspecial_tokens)\n\u001b[0;32m---> 23\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain_from_iterator(\u001b[43mseq_structs_white_space\u001b[49m, trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Create a fast tokenizer\u001b[39;00m\n\u001b[1;32m     26\u001b[0m fast_tokenizer \u001b[38;5;241m=\u001b[39m PreTrainedTokenizerFast(\n\u001b[1;32m     27\u001b[0m     tokenizer_object\u001b[38;5;241m=\u001b[39mtokenizer, \n\u001b[1;32m     28\u001b[0m     model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m84\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     35\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seq_structs_white_space' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"N\"), )\n",
    "special_tokens = [\"[PAD]\", \"N\", \"[CLS]\", \"[MASK]\", \"[SEP]\"]\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.mask_token = \"[MASK]\"\n",
    "tokenizer.cls_token = \"[CLS]\"\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.unknown_token = \"N\"\n",
    "tokenizer.sep_token = \"[SEP]\"\n",
    "tokenizer.model_max_length = 84\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "\n",
    "# Post processor\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[PAD]\", 0), (\"N\", 1), (\"[CLS]\", 2), (\"[MASK]\", 3), (\"[SEP]\", 4)]\n",
    ")\n",
    "\n",
    "# Train the tokenizer\n",
    "trainer = BpeTrainer(special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(seq_structs_white_space, trainer=trainer)\n",
    "\n",
    "# Create a fast tokenizer\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer, \n",
    "    model_max_length=84, \n",
    "    sep_token=\"[SEP]\", \n",
    "    cls_token=\"[CLS]\", \n",
    "    unk_token=\"N\", \n",
    "    pad_token=\"[PAD]\", \n",
    "    mask_token=\"[MASK]\", \n",
    "    return_special_tokens_mask=1\n",
    ")\n",
    "\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc5b5088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T': 11,\n",
       " 'N': 1,\n",
       " '[CLS]': 2,\n",
       " '.': 7,\n",
       " 'G': 10,\n",
       " '[MASK]': 3,\n",
       " ')': 6,\n",
       " '[SEP]': 4,\n",
       " '[PAD]': 0,\n",
       " 'C': 9,\n",
       " '(': 5,\n",
       " 'A': 8}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "776f0f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/tokenizers/seq_struct_sep_whitespace/tokenizer_config.json',\n",
       " '../data/tokenizers/seq_struct_sep_whitespace/special_tokens_map.json',\n",
       " '../data/tokenizers/seq_struct_sep_whitespace/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.save_pretrained('../data/tokenizers/seq_struct_sep_whitespace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26c55501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (85 > 84). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized_structures = fast_tokenizer(seq_structs_white_space, padding=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "edfcb825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='../data/tokenizers/seq_struct_whitespace', vocab_size=11, model_max_length=83, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': 'N', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"N\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('../data/tokenizers/seq_struct_sep_whitespace')\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6e70350",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_seq_structs = tokenizer(seq_structs_white_space, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8cef9304",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_seq_structs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenized_seq_structs\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_seq_structs' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_seq_structs.input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45ac6345-11c5-44bb-8ffc-56c3186c09b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of aptamer_transformer.model failed: Traceback (most recent call last):\n",
      "  File \"/glade/work/mlsample/conda-envs/guess/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/glade/work/mlsample/conda-envs/guess/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/glade/work/mlsample/conda-envs/guess/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/glade/work/mlsample/conda-envs/guess/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 365, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/glade/work/mlsample/conda-envs/guess/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 319, in update_instances\n",
      "    refs = gc.get_referrers(old)\n",
      "KeyboardInterrupt\n",
      "]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m      4\u001b[0m cfg\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device,\n\u001b[1;32m      6\u001b[0m })\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# with open('../data/raw_data/nupack_strucutre_data/mfe.pickle', 'rb') as f:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     mfe = pickle.load(f)\u001b[39;00m\n",
      "File \u001b[0;32m/glade/work/mlsample/aptamer_transformer/aptamer_transformer/factories_model_loss.py:125\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_config:\n\u001b[1;32m    124\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid model config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/glade/work/mlsample/aptamer_transformer/aptamer_transformer/model.py:164\u001b[0m, in \u001b[0;36mTransformerEncoderRegression.__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTransformerEncoderRegression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg \u001b[38;5;241m=\u001b[39m cfg\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "cfg = read_cfg('../aptamer_transformer/config.yaml')\n",
    "# Initialize the model and optimizer here as you did during training\n",
    "device = torch.device(\"cuda:0\")  \n",
    "cfg.update({\n",
    "    'device': device,\n",
    "})\n",
    "\n",
    "model = get_model(cfg).to(device)\n",
    "\n",
    "# with open('../data/raw_data/nupack_strucutre_data/mfe.pickle', 'rb') as f:\n",
    "#     mfe = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4e5fa412",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/mlsample/conda-envs/guess/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/mlsample/conda-envs/guess/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/glade/work/mlsample/aptamer_transformer/aptamer_transformer/model.py:255\u001b[0m, in \u001b[0;36mStructTransformerEncoderRegression.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 255\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m    259\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n",
      "File \u001b[0;32m/glade/work/mlsample/conda-envs/guess/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/mlsample/conda-envs/guess/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/glade/work/mlsample/aptamer_transformer/aptamer_transformer/model.py:89\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     91\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(x)\n\u001b[1;32m     92\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_position_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "x = tokenized_seq_structs.input_ids[:10]\n",
    "attn_mask = tokenized_seq_structs.attention_mask[:10]\n",
    "\n",
    "outputs = model(x, attn_mask=attn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22085963",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = read_cfg('../aptamer_transformer/config.yaml')\n",
    "\n",
    "dna_dataset = load_dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "662916c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0, 'N': 1, '[CLS]': 2, '(': 4, ')': 5, '.': 6, '[MASK]': 3}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9b6cae1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]. (. ( (..... ) ). ) ( ( (.......... ) ) ).......... [PAD]'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dna_dataset.tokenized_struc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139cdb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "struc_energy = mfe[0][0]\n",
    "\n",
    "energy = struc_energy.energy\n",
    "struc = struc_energy.structure\n",
    "\n",
    "print(f'Dot Bracket Secondart Strucutre Notation:\\n{struc.dotparensplus()}\\n')\n",
    "print(f'Adjacency Matrix (edges):\\n{struc.matrix()}\\n')\n",
    "print(f'Secondary Strucutre Mean Free Energy:\\n{energy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_structures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npl-2023b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
