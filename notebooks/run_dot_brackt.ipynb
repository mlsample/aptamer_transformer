{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d80798e-20bd-401f-a156-9577573273fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-18 13:55:54.310258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-18 13:55:54.310324: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-18 13:55:54.311378: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-18 13:55:54.317641: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-18 13:55:55.266162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import quantile_transform \n",
    "from x_transformers import XTransformer, TransformerWrapper, Decoder, Encoder, ViTransformerWrapper\n",
    "\n",
    "from aptamer_transformer.model import *\n",
    "from aptamer_transformer.factories_model_loss import *\n",
    "from aptamer_transformer.data_utils import *\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880100f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../data/saved_processed_data/saved_h5/seq_struct_n_classes_2.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf97ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = df.dot_bracket_struc.apply(lambda x: ' '.join(x)).values.tolist()\n",
    "seqs = df.Sequence.apply(lambda x: ' '.join(x)).values.tolist()\n",
    "\n",
    "seq_structs = [f'{seq}{struct}' for seq, struct in zip(df.Sequence, df.dot_bracket_struc)]\n",
    "\n",
    "seq_structs_white_space = [(seq , struct) for seq, struct in zip(seqs, structures)]\n",
    "\n",
    "len_seq_structs = [len(struct) for struct in seq_structs]\n",
    "print(max(len_seq_structs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3983f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = read_cfg('../aptamer_transformer/config.yaml')\n",
    "\n",
    "dna_dataset = load_dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecd3c2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  8, 11,  ...,  7,  7,  4],\n",
       "        [ 2, 11,  9,  ...,  4,  0,  0],\n",
       "        [ 2,  8, 10,  ...,  4,  0,  0],\n",
       "        ...,\n",
       "        [ 2, 10, 10,  ...,  4,  0,  0],\n",
       "        [ 2, 11, 10,  ...,  4,  0,  0],\n",
       "        [ 2, 11, 10,  ...,  4,  0,  0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_dataset.tokenized_seq_struc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f26d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    pass\n",
    "\n",
    "args = Object()\n",
    "args.distributed = False\n",
    "train_loader, val_loader, test_loader, train_sampler = get_data_loaders(dna_dataset, cfg, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a074c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d305fc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['A G G A T T A T C G T C C G G G G G G G G G G C C C G G T T T C G G G G T C G T',\n",
       "        '. . . . . . . . . . . ( ( ( ( ( . . . . . . . ) ) ) ) ) . . . ( ( . . . . ) ) .'],\n",
       "       ['G G G G G G G G G G G C G G G C T G G C T T T A T T C C G G T T T C G T G T T T',\n",
       "        '. . . . . . . . . . . ( ( ( . . . . . . . . . . . . ) ) ) . . . . . . . . . . .'],\n",
       "       ['G G G C G G G C C C C C C C C G C A A G G G G G T C G T T T C T A C A G T G G G',\n",
       "        '( ( ( . . . . ) ) ) ( ( ( ( ( . . . . ) ) ) ) ) . . . . . . . . . . . . . . . .'],\n",
       "       ...,\n",
       "       ['G G G G G G G C G C G C C C C G G G G C T A G G T G T A C G C T C G G T T T A G',\n",
       "        '. . ( ( ( ( . . . . . ) ) ) ) . . . . . . . . . . . . . . . . . . . . . . . . .'],\n",
       "       ['G G G G C C C C C C C C C C C C C G C C C A C T C T C G G T G A C G C A C A G C',\n",
       "        '( ( ( ( . . . . . ) ) ) ) . . ( ( ( . . . . . . . . ) ) ) ( ( . . . . . ) ) . .'],\n",
       "       ['G G G C G G G A G G G A G G G G G G G G G T G C C G C G T C T G T T C C C T A T A',\n",
       "        '. . . . ( ( ( ( . . ( ( . ( . ( ( . . . . . . ) ) . ) . ) ) . . . ) ) ) ) . . . .']],\n",
       "      dtype='<U81')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(batch[0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46f0bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] A T C G G G G G G T G G G T T T G G G T A T G G G G T C T G C A C T A T G G C T C [SEP]............................. ( (...... ) ).. [SEP]',\n",
       " '[CLS] T C G C G G G G G G C G G G T C G G G T G C T C G T T C G A G G G G T C G C A G [SEP]. (. ( (..... ) ). ) ( ( (.......... ) ) ).......... [SEP] [PAD] [PAD]',\n",
       " '[CLS] A G G T T G G T G T A G G T T T A G G T T G T A T T G G T T A G G G T T G A G A [SEP]........................................ [SEP] [PAD] [PAD]',\n",
       " '[CLS] T G G C T G T C T T A C G A T C G C G A A G G A C G A T T G A A A A C T T T G A [SEP]............. ( ( ( (....... ) ) ) )............ [SEP] [PAD] [PAD]',\n",
       " '[CLS] G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G [SEP]........................................ [SEP] [PAD] [PAD]',\n",
       " '[CLS] A T A G T T A C G G G T G A G C C G T C A T T C A A G T T T A C T A C T T C T G [SEP]...... ( ( ( (..... ) ) ) )..... ( ( ( (..... ) ) ) )... [SEP] [PAD] [PAD]',\n",
       " '[CLS] C C C A T G G T A G G T A T T G C T T G G T A G G G A T A G T G G G C T T G A T G [SEP]. ( ( (. (.......... ). ) ) ) ( (.... ) )............ [SEP]',\n",
       " '[CLS] G G G A G G G A G G G T G G G G G G T T C T C G C T G C G G G T T T T G G T G C [SEP]. (. ( (. ( ( (........... ) ) ). ) ). )............ [SEP] [PAD] [PAD]',\n",
       " '[CLS] T G G G T G G G A G G G A G G G G G G C T T T A T T C C G G T G T T T T T T C G [SEP]........................................ [SEP] [PAD] [PAD]',\n",
       " '[CLS] A G G C T T A T C G T C A G G G G G G T G G G T C C G G G T T C G G G G T C G T [SEP]........................ ( ( (.... ) ) )...... [SEP] [PAD] [PAD]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna_dataset.tokenizer.batch_decode(dna_dataset.tokenized_seq_struc[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"N\"), )\n",
    "special_tokens = [\"[PAD]\", \"N\", \"[CLS]\", \"[MASK]\", \"[SEP]\"]\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.mask_token = \"[MASK]\"\n",
    "tokenizer.cls_token = \"[CLS]\"\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "tokenizer.unknown_token = \"N\"\n",
    "tokenizer.sep_token = \"[SEP]\"\n",
    "tokenizer.model_max_length = 85\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "\n",
    "# Post processor\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[PAD]\", 0), (\"N\", 1), (\"[CLS]\", 2), (\"[MASK]\", 3), (\"[SEP]\", 4)]\n",
    ")\n",
    "\n",
    "# Train the tokenizer\n",
    "trainer = BpeTrainer(special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(seq_structs_white_space, trainer=trainer)\n",
    "\n",
    "# Create a fast tokenizer\n",
    "fast_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer, \n",
    "    model_max_length=85, \n",
    "    sep_token=\"[SEP]\", \n",
    "    cls_token=\"[CLS]\", \n",
    "    unk_token=\"N\", \n",
    "    pad_token=\"[PAD]\", \n",
    "    mask_token=\"[MASK]\", \n",
    "    return_special_tokens_mask=1\n",
    ")\n",
    "\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer.save_pretrained('../data/tokenizers/seq_struct_sep_whitespace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac6345-11c5-44bb-8ffc-56c3186c09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = read_cfg('../aptamer_transformer/config.yaml')\n",
    "# Initialize the model and optimizer here as you did during training\n",
    "device = torch.device(\"cuda:0\")  \n",
    "cfg.update({\n",
    "    'device': device,\n",
    "})\n",
    "\n",
    "model = get_model(cfg).to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npl-2023b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
